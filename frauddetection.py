# -*- coding: utf-8 -*-
"""scienaptic2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oCqVFq43RXQpwjGOEFB_RHaruUeA87Af
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from scipy.stats import skew
from scipy.stats import norm
from datetime import date
import datetime as dt
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from collections import Counter

train_df = pd.read_csv('/content/fraudTrain.csv')
test_df = pd.read_csv('/content/fraudTest.csv')

test_df.head()

print(test_df.shape),print(train_df.shape)

# Assuming your datetime column is named 'trans_date_trans_time'
train_df['trans_date_trans_time'] = pd.to_datetime(train_df['trans_date_trans_time'])

train_df['year'] = train_df['trans_date_trans_time'].dt.year
train_df['month'] = train_df['trans_date_trans_time'].dt.month
train_df['day'] = train_df['trans_date_trans_time'].dt.day
train_df['hour'] = train_df['trans_date_trans_time'].dt.hour
train_df['minute'] = train_df['trans_date_trans_time'].dt.minute

# Do the same for the 'test' DataFrame
test_df['trans_date_trans_time'] = pd.to_datetime(test_df['trans_date_trans_time'])

test_df['year'] = test_df['trans_date_trans_time'].dt.year
test_df['month'] = test_df['trans_date_trans_time'].dt.month
test_df['day'] = test_df['trans_date_trans_time'].dt.day
test_df['hour'] = test_df['trans_date_trans_time'].dt.hour

train_df['dob'] = pd.to_datetime(train_df['dob'])
train_df['age'] = (train_df['trans_date_trans_time'] - train_df['dob']).dt.days // 365 # Assuming 'trans_date_trans_time' is already datetime

test_df['dob'] = pd.to_datetime(test_df['dob'])
test_df['age'] = (test_df['trans_date_trans_time'] - test_df['dob']).dt.days // 365

train_df = train_df.drop(['first', 'last', 'gender', 'street', 'city', 'state', 'zip', ], axis=1)
test_df = test_df.drop(['first', 'last', 'gender', 'street', 'city', 'state', 'zip'], axis=1)
train_df = train_df.drop(['cc_num', 'unix_time', 'trans_num', 'Unnamed: 0','merch_lat',	'merch_long'], axis=1)
test_df = test_df.drop(['cc_num', 'unix_time', 'trans_num', 'Unnamed: 0','merch_lat',	'merch_long'], axis=1)

from sklearn.preprocessing import LabelEncoder

# Assuming 'train_df' is your DataFrame
# Select only object type columns which are typically categorical
categorical_cols = train_df.select_dtypes(include=['object']).columns

encoder = LabelEncoder()

for col in categorical_cols:
  train_df[col] = encoder.fit_transform(train_df[col])

# Now, your selected categorical columns in 'train_df' DataFrame are label encoded.
# You can display the first few rows to see the changes
train_df.head()

# Do the same for the 'test_df' DataFrame
categorical_cols_test = test_df.select_dtypes(include=['object']).columns # Use a different variable name

encoder_test = LabelEncoder() # Use a different encoder instance for test

for col in categorical_cols_test:
  test_df[col] = encoder_test.fit_transform(test_df[col]) # Fit and transform on test

# Now, your selected categorical columns in 'test_df' DataFrame are label encoded.
# You can display the first few rows to see the changes
test_df.head()

train_df = train_df.drop(['trans_date_trans_time', 'dob','minute'], axis=1)
test_df = test_df.drop(['trans_date_trans_time', 'dob'], axis=1)

# Assuming 'train_df' is your DataFrame
correlation_matrix = train_df.corr()

plt.figure(figsize=(12, 10)) # You can adjust the figure size as needed
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Training Data')
plt.show()

#autoencoder

# Core libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Scikit-learn utilities
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_curve, auc, confusion_matrix, roc_auc_score
)

# TensorFlow / Keras for autoencoder
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

# Assuming 'is_fraud' is your target variable column name
X_full_train = train_df.drop('is_fraud', axis=1)
y_full_train = train_df['is_fraud']

X_test = test_df.drop('is_fraud', axis=1)
y_test = test_df['is_fraud']

# We'll use the entire training set for filtering in the autoencoder part,
# so no need to split X_full_train further here for now.
X_train, y_train = X_full_train, y_full_train

print(X_train.shape)
print(X_test.shape)
X_train.columns

X_test.head()

X_train_normal = X_train[y_train == 0]

# Scale data
scaler_ae = MinMaxScaler()
X_train_normal_scaled = scaler_ae.fit_transform(X_train_normal)
X_test_scaled = scaler_ae.transform(X_test)

# Get number of features
input_dim = X_train_normal_scaled.shape[1]
print(X_train_normal_scaled.shape)

# Autoencoder architecture
encoding_dim1 = max(16, int(input_dim * 0.75))
encoding_dim2 = max(8, int(input_dim * 0.5))
bottleneck_dim = max(4, int(input_dim * 0.25))

simple_autoencoder = Sequential([
    Dense(encoding_dim1, activation='relu', input_shape=(input_dim,)),
    Dropout(0.1),
    Dense(encoding_dim2, activation='relu'),
    Dropout(0.1),
    Dense(bottleneck_dim, activation='relu', name='bottleneck_layer'),
    Dense(encoding_dim2, activation='relu'),
    Dropout(0.1),
    Dense(encoding_dim1, activation='relu'),
    Dropout(0.1),
    Dense(input_dim, activation='sigmoid')
])

# Compile model
simple_autoencoder.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')
simple_autoencoder.summary()

# Early stopping to avoid overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Train on normal (non-fraud) data
history = simple_autoencoder.fit(
    X_train_normal_scaled, X_train_normal_scaled,
    epochs=100, batch_size=32, validation_split=0.1,
    shuffle=True, callbacks=[early_stopping], verbose=1
)

# Plot training history
plt.figure(figsize=(10, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Autoencoder Training Loss')
plt.legend()
plt.grid(True)
plt.show()

# Predict and compute MSE (error) for both train and test sets
reconstructions_train = simple_autoencoder.predict(X_train_normal_scaled)
mse_train_normal = np.mean((X_train_normal_scaled - reconstructions_train) ** 2, axis=1)

reconstructions_test = simple_autoencoder.predict(X_test_scaled)
mse_test = np.mean((X_test_scaled - reconstructions_test) ** 2, axis=1)

# Separate test errors by class
mse_test_normal = mse_test[y_test == 0]
mse_test_fraud = mse_test[y_test == 1]

plt.figure(figsize=(12, 7))
sns.histplot(mse_train_normal, bins=50, kde=True, label='Normal Train', color='blue')
sns.histplot(mse_test_normal, bins=50, kde=True, label='Normal Test', color='green')
sns.histplot(mse_test_fraud, bins=50, kde=True, label='Fraud Test', color='red')
plt.xlabel('Reconstruction Error (MSE)')
plt.ylabel('Frequency')
plt.title('Reconstruction Error Distribution')
plt.legend()
plt.grid(True)
plt.show()

# ROC to find best threshold
fpr, tpr, thresholds = roc_curve(y_test, mse_test)
roc_auc = auc(fpr, tpr)

# Youden's J statistic
optimal_idx = np.argmax(tpr - fpr)
optimal_threshold = thresholds[optimal_idx]

final_threshold = optimal_threshold

# Predict anomalies
y_pred = (mse_test > final_threshold).astype(int)

# Evaluation metrics
acc = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred, pos_label=1, zero_division=0)
rec = recall_score(y_test, y_pred, pos_label=1, zero_division=0)
f1 = f1_score(y_test, y_pred, pos_label=1, zero_division=0)
roc_auc_score_final = roc_auc_score(y_test, mse_test)
cm = confusion_matrix(y_test, y_pred)

# Print results
print(f"Accuracy: {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall: {rec:.4f}")
print(f"F1-score: {f1:.4f}")
print(f"ROC AUC: {roc_auc_score_final:.4f}")
print("Confusion Matrix:\n", cm)

# Confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['Predicted Normal', 'Predicted Fraud'],
            yticklabels=['Actual Normal', 'Actual Fraud'])
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.show()

# ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--', color='navy')
plt.scatter(fpr[optimal_idx], tpr[optimal_idx], color='red', s=100, label=f'Optimal Threshold ({optimal_threshold:.4f})')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid(True)
plt.show()

import pandas as pd
from catboost import CatBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    f1_score, roc_auc_score, classification_report, confusion_matrix
)

#catboost
# Ensure target column exists
if 'is_fraud' not in train_df.columns or 'is_fraud' not in test_df.columns:
    raise ValueError("Target column 'is_fraud' is missing.")

# Separate features and target
X_full_train = train_df.drop('is_fraud', axis=1)
y_full_train = train_df['is_fraud']

X_test_cb = test_df.drop('is_fraud', axis=1)
y_test_cb = test_df['is_fraud']
x

# Keep only common columns and ensure same order
common_cols = [col for col in X_full_train.columns if col in X_test_cb.columns]
X_full_train = X_full_train[common_cols]
X_test_cb = X_test_cb[common_cols]

# Sanity check
if not all(X_full_train.columns == X_test_cb.columns):
    print("Warning: Column mismatch between train and test. Re-aligning strictly.")
    X_test_cb = X_test_cb[X_full_train.columns]

# Summary
print(f"\nTraining shape: {X_full_train.shape}")
print(f"Test shape: {X_test_cb.shape}")
print("Features used:", X_full_train.columns.tolist())

# Define your categorical columns
user_defined_label_encoded_cat_cols = ['merchant', 'category', 'job']

# Filter for valid columns
valid_cat_cols = [col for col in user_defined_label_encoded_cat_cols if col in X_full_train.columns]
cat_features_indices = [X_full_train.columns.get_loc(col) for col in valid_cat_cols]

# Warnings for missing columns
missing = set(user_defined_label_encoded_cat_cols) - set(valid_cat_cols)
if missing:
    print(f"Warning: These categorical columns were not found: {missing}")

print(f"\nCategorical Features: {valid_cat_cols}")
print(f"Indices for CatBoost: {cat_features_indices}")

# Split with stratification for class balance
X_train, X_eval, y_train, y_eval = train_test_split(
    X_full_train, y_full_train, test_size=0.15, stratify=y_full_train, random_state=42
)

# Distribution check
print(f"\nTrain shape: {X_train.shape}, Eval shape: {X_eval.shape}")
print(f"Train class distribution: {y_train.value_counts().to_dict()}")
if y_train.value_counts().get(1, 0) == 0:
    print("Warning: No positive (fraud) samples in training data.")

# Initialize CatBoost with class balancing
cb_model_no_tune = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.05,
    depth=6,
    l2_leaf_reg=3,
    auto_class_weights='Balanced',
    random_seed=42,
    verbose=100,
    eval_metric='F1'
)

# Set categorical features
if cat_features_indices:
    cb_model_no_tune.set_params(cat_features=cat_features_indices)

# Train model
print("\nTraining CatBoost model...")
cb_model_no_tune.fit(
    X_train, y_train,
    eval_set=(X_eval, y_eval),
    early_stopping_rounds=50
)

# Report best iteration
best_iter = cb_model_no_tune.get_best_iteration()
best_f1 = cb_model_no_tune.get_evals_result()['validation']['F1'][-1]
print(f"\nTraining complete. Best Iteration: {best_iter}, Best Eval F1: {best_f1:.4f}")

print("\n--- Evaluation on Full Training Set ---")
train_preds = cb_model_no_tune.predict(X_full_train)
train_proba = cb_model_no_tune.predict_proba(X_full_train)[:, 1]

print("Confusion Matrix:")
print(confusion_matrix(y_full_train, train_preds))
print("\nClassification Report:")
print(classification_report(y_full_train, train_preds, target_names=['Non-Fraud (0)', 'Fraud (1)'], zero_division=0))

try:
    auc_train = roc_auc_score(y_full_train, train_proba)
    print(f"ROC AUC (Train): {auc_train:.4f}")
except ValueError as e:
    print(f"Could not calculate ROC AUC: {e}")

#evalutaion
print("\n--- Evaluation on Test Set ---")
test_preds = cb_model_no_tune.predict(X_test_cb)
test_proba = cb_model_no_tune.predict_proba(X_test_cb)[:, 1]

print("Confusion Matrix:")
print(confusion_matrix(y_test_cb, test_preds))
print("\nClassification Report:")
print(classification_report(y_test_cb, test_preds, target_names=['Non-Fraud (0)', 'Fraud (1)'], zero_division=0))

try:
    auc_test = roc_auc_score(y_test_cb, test_proba)
    print(f"ROC AUC (Test): {auc_test:.4f}")
except ValueError as e:
    print(f"Could not calculate ROC AUC: {e}")

print("\n--- Feature Importances ---")
try:
    importances = cb_model_no_tune.get_feature_importance(prettified=True)
    print(importances)
except Exception as e:
    print(f"Could not retrieve feature importances: {e}")

